{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Akshay Reddy Akkati\n",
    "# ID: YX50097\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# importing the MNIST dataset from scikit learn\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# scale\n",
    "X = X / 255\n",
    "\n",
    "# For testing on XOR I used following as data\n",
    "# inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# expected_output = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# one-hot encoding on categorical data\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "# split the data into train, dev and test in the ratio of 70, 20 and 10 \n",
    "# number of train records\n",
    "n = 49000\n",
    "#  total number of train and dev records\n",
    "m = 63000\n",
    "\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_dev, X_test = X[:n].T, X[n:m].T, X[m:].T\n",
    "Y_train, Y_dev, Y_test = Y_new[:,:n], Y_new[:, n:m], Y_new[:,m:]\n",
    "shuffle_index = np.random.permutation(n)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def tan_h(z):\n",
    "    res = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    return res\n",
    "\n",
    "def compute_loss(Y, Y_hat):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "    return L\n",
    "\n",
    "def forward_propagate(X, params):\n",
    "\n",
    "    cache = {}\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "#     cache[\"A1\"] = tan_h(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def back_propagate(X, Y, params, cache):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "#     dZ1 = dA1 * tan_h(cache[\"Z1\"]) * (1 - tan_h(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(138)\n",
    "# np.random.seed(2)\n",
    "# w1 = np.random.rand(n_h,n_x)\n",
    "# w2 = np.random.rand(n_y,n_h)\n",
    "\n",
    "# hyperparameters\n",
    "n_x = X_train.shape[0]\n",
    "# number of neurons for the model\n",
    "number_of_neurons = 56\n",
    "# learning_rate = the rate at which the model should be learned\n",
    "learning_rate = 0.3\n",
    "beta = .9\n",
    "batch_size = 200\n",
    "# batches = batch_size = the size of batches of inputs\n",
    "batches = -(-m // batch_size)\n",
    "# epochs=number of iterations\n",
    "epochs = 20\n",
    "\n",
    "# initialization\n",
    "params = { \"W1\": np.random.randn(number_of_neurons, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((number_of_neurons, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(digits, number_of_neurons) * np.sqrt(1. / number_of_neurons),\n",
    "           \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / number_of_neurons) }\n",
    "\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "\n",
    "# train\n",
    "for i in range(epochs):\n",
    "\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "    \n",
    "#     :::::For testing XOR data:::::\n",
    "#     z1,a1,z2,a2 = forward_prop(w1,w2,x)\n",
    "#     loss = compute_loss(y,a2)\n",
    "#     losses.append(loss)\n",
    "#     da2,dw2,dz1,dw1 = back_prop(m,w1,w2,z1,a1,z2,a2,y)\n",
    "#     w2 = w2-lr*dw2\n",
    "#     w1 = w1-lr*dw1\n",
    "# def predict(w1,w2,input):\n",
    "#     z1,a1,z2,a2 = forward_prop(w1,w2,test)\n",
    "#     a2 = np.squeeze(a2)\n",
    "#     if a2>=0.5:\n",
    "#         print( \"1\")\n",
    "#     else:\n",
    "#         print(\"0\")\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        cache = forward_propagate(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "        params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "    cache = forward_propagate(X_train, params)\n",
    "    train_loss = compute_loss(Y_train, cache[\"A2\"])\n",
    "    cache = forward_propagate(X_dev, params)\n",
    "    dev_loss = compute_loss(Y_dev, cache[\"A2\"])\n",
    "    print(\"At Epoch:\" + str(i+1) + \" train-loss = \" + str(train_loss) + \" & dev-loss = \" + str(dev_loss))\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Testing the model on test data and printing the report\n",
    "cache = forward_propagate(X_test, params)\n",
    "predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
